{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52a9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.9.0-cp310-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.24.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.9 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.9.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.9 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/ashithrai/Library/Python/3.10/lib/python/site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-12.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Using cached torch-2.9.0-cp310-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "Using cached torchvision-0.24.0-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached torchaudio-2.9.0-cp310-cp310-macosx_11_0_arm64.whl (806 kB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached pillow-12.0.0-cp310-cp310-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl (5.3 MB)\n",
      "Installing collected packages: mpmath, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [torchaudio]2\u001b[0m [torchaudio]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 filelock-3.20.0 fsspec-2025.10.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 pillow-12.0.0 sympy-1.14.0 torch-2.9.0 torchaudio-2.9.0 torchvision-0.24.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5598d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "689d3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fdbcc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.layer1 = ConvBlock(channels, channels)\n",
    "        self.layer2 = ConvBlock(channels, channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer2(self.layer1(x))\n",
    "        return out + x  # skip connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf2cfe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Stage 1\n",
    "        self.layer1 = ConvBlock(3, 32, stride=2)\n",
    "        \n",
    "        # Stage 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConvBlock(32, 64, stride=2),\n",
    "            ResidualBlock(64)\n",
    "        )\n",
    "        \n",
    "        # Stage 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ConvBlock(64, 128, stride=2),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128)\n",
    "        )\n",
    "        \n",
    "        # Stage 4\n",
    "        self.layer4 = nn.Sequential(\n",
    "            ConvBlock(128, 256, stride=2),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256)\n",
    "        )\n",
    "        \n",
    "        # Stage 5\n",
    "        self.layer5 = nn.Sequential(\n",
    "            ConvBlock(256, 512, stride=2),\n",
    "            ResidualBlock(512),\n",
    "            ResidualBlock(512)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer1(x)  # 320×320×32\n",
    "        x2 = self.layer2(x1) # 160×160×64\n",
    "        x3 = self.layer3(x2) # 80×80×128\n",
    "        x4 = self.layer4(x3) # 40×40×256\n",
    "        x5 = self.layer5(x4) # 20×20×512\n",
    "        return x3, x4, x5  # multiple scales for detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "007dbe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 80, 80])\n",
      "torch.Size([1, 256, 40, 40])\n",
      "torch.Size([1, 512, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.randn(1, 3, 640, 640)  # one sample\n",
    "model = MyBackbone()\n",
    "f3, f4, f5 = model(x)\n",
    "\n",
    "print(f3.shape)  # (1, 128, 80, 80)\n",
    "print(f4.shape)  # (1, 256, 40, 40)\n",
    "print(f5.shape)  # (1, 512, 20, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fd95e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neck (FPN) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79ce56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class FPNNeck(nn.Module):\n",
    "    def __init__(self, channels=[128, 256, 512]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Reduce channels before fusion\n",
    "        self.reduce_c3 = nn.Conv2d(channels[0], 128, 1)\n",
    "        self.reduce_c4 = nn.Conv2d(channels[1], 256, 1)\n",
    "        self.reduce_c5 = nn.Conv2d(channels[2], 512, 1)\n",
    "\n",
    "        # FIXED: Input channels are sums of concatenated feature maps\n",
    "        self.conv_c4 = ConvBlock(512 + 256, 256)  # up_c5(512) + c4(256)\n",
    "        self.conv_c3 = ConvBlock(256 + 128, 128)  # up_p4(256) + c3(128)\n",
    "\n",
    "    def forward(self, c3, c4, c5):\n",
    "        # Step 1: Channel reduction\n",
    "        c3 = self.reduce_c3(c3)  # (128 channels)\n",
    "        c4 = self.reduce_c4(c4)  # (256 channels)\n",
    "        c5 = self.reduce_c5(c5)  # (512 channels)\n",
    "\n",
    "        # Step 2: Top-down fusion\n",
    "        up_c5 = F.interpolate(c5, scale_factor=2, mode='nearest')\n",
    "        fused_c4 = torch.cat([up_c5, c4], dim=1)\n",
    "        p4 = self.conv_c4(fused_c4)\n",
    "\n",
    "        up_p4 = F.interpolate(p4, scale_factor=2, mode='nearest')\n",
    "        fused_c3 = torch.cat([up_p4, c3], dim=1)\n",
    "        p3 = self.conv_c3(fused_c3)\n",
    "\n",
    "        # Step 3: Return multi-scale outputs\n",
    "        return p3, p4, c5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "934e008e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p3: torch.Size([1, 128, 80, 80])\n",
      "p4: torch.Size([1, 256, 40, 40])\n",
      "p5: torch.Size([1, 512, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "backbone = MyBackbone()\n",
    "neck = FPNNeck(channels=[128, 256, 512])\n",
    "\n",
    "x = torch.randn(1, 3, 640, 640)\n",
    "f3, f4, f5 = backbone(x)\n",
    "p3, p4, p5 = neck(f3, f4, f5)\n",
    "\n",
    "print(\"p3:\", p3.shape)\n",
    "print(\"p4:\", p4.shape)\n",
    "print(\"p5:\", p5.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2286c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, num_classes=1, anchors_per_scale=3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.anchors_per_scale = anchors_per_scale\n",
    "        self.num_outputs = 5 + num_classes  # [x, y, w, h, obj, class]\n",
    "\n",
    "        # 1x1 conv for each scale\n",
    "        self.head_small = nn.Conv2d(128, anchors_per_scale * self.num_outputs, 1)\n",
    "        self.head_medium = nn.Conv2d(256, anchors_per_scale * self.num_outputs, 1)\n",
    "        self.head_large = nn.Conv2d(512, anchors_per_scale * self.num_outputs, 1)\n",
    "\n",
    "    def forward(self, p3, p4, p5):\n",
    "        # predictions for each scale\n",
    "        out_small = self.head_small(p3)\n",
    "        out_medium = self.head_medium(p4)\n",
    "        out_large = self.head_large(p5)\n",
    "        return [out_small, out_medium, out_large]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4bd7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BacteriaDetector(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.backbone = MyBackbone()\n",
    "        self.neck = FPNNeck(channels=[128, 256, 512])\n",
    "        self.head = DetectionHead(num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1️⃣ Extract features\n",
    "        c3, c4, c5 = self.backbone(x)\n",
    "        # 2️⃣ Fuse features\n",
    "        p3, p4, p5 = self.neck(c3, c4, c5)\n",
    "        # 3️⃣ Predict\n",
    "        preds = self.head(p3, p4, p5)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9035965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale 1: torch.Size([1, 18, 80, 80])\n",
      "Scale 2: torch.Size([1, 18, 40, 40])\n",
      "Scale 3: torch.Size([1, 18, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 640, 640)\n",
    "model = BacteriaDetector(num_classes=1)\n",
    "preds = model(x)\n",
    "\n",
    "for i, p in enumerate(preds):\n",
    "    print(f\"Scale {i+1}: {p.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36f4c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def bbox_ciou(box1, box2, eps=1e-7):\n",
    "    \"\"\"\n",
    "    box1, box2: [N, 4] in (x_center, y_center, width, height)\n",
    "    Returns: CIoU loss for each pair\n",
    "    \"\"\"\n",
    "    # Convert boxes to corner coordinates\n",
    "    b1_x1, b1_y1 = box1[:, 0] - box1[:, 2] / 2, box1[:, 1] - box1[:, 3] / 2\n",
    "    b1_x2, b1_y2 = box1[:, 0] + box1[:, 2] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "    b2_x1, b2_y1 = box2[:, 0] - box2[:, 2] / 2, box2[:, 1] - box2[:, 3] / 2\n",
    "    b2_x2, b2_y2 = box2[:, 0] + box2[:, 2] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "\n",
    "    # Intersection area\n",
    "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
    "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    "    # Union area\n",
    "    union = (b1_x2 - b1_x1) * (b1_y2 - b1_y1) + (b2_x2 - b2_x1) * (b2_y2 - b2_y1) - inter + eps\n",
    "    iou = inter / union\n",
    "\n",
    "    # Center distance\n",
    "    cdist = (box1[:, 0] - box2[:, 0]) ** 2 + (box1[:, 1] - box2[:, 1]) ** 2\n",
    "\n",
    "    # Enclosing box diagonal distance\n",
    "    c_x1, c_y1 = torch.min(b1_x1, b2_x1), torch.min(b1_y1, b2_y1)\n",
    "    c_x2, c_y2 = torch.max(b1_x2, b2_x2), torch.max(b1_y2, b2_y2)\n",
    "    c_diag = (c_x2 - c_x1) ** 2 + (c_y2 - c_y1) ** 2 + eps\n",
    "\n",
    "    # Aspect ratio term\n",
    "    v = (4 / (3.14159265 ** 2)) * torch.pow(torch.atan(b2_x2 / (b2_y2 + eps)) - torch.atan(b1_x2 / (b1_y2 + eps)), 2)\n",
    "    with torch.no_grad():\n",
    "        alpha = v / (1 - iou + v + eps)\n",
    "\n",
    "    ciou = iou - (cdist / c_diag + alpha * v)\n",
    "    return 1 - ciou  # CIoU loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3b9f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "bce = nn.BCELoss(reduction='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9def7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_loss(preds, targets, anchors, device, lambda_box=5.0, lambda_obj=1.0, lambda_cls=1.0):\n",
    "    \"\"\"\n",
    "    preds: list of prediction tensors from detection head\n",
    "    targets: list of ground truth [class, x, y, w, h]\n",
    "    anchors: anchor sizes per scale\n",
    "    \"\"\"\n",
    "    total_box_loss = 0\n",
    "    total_obj_loss = 0\n",
    "    total_cls_loss = 0\n",
    "\n",
    "    for scale_i, pred in enumerate(preds):\n",
    "        B, C, H, W = pred.shape\n",
    "        pred = pred.view(B, 3, (5 + 1), H, W).permute(0, 1, 3, 4, 2)\n",
    "        # pred shape → [B, 3, H, W, 6] for 1 class (x, y, w, h, obj, class)\n",
    "\n",
    "        # Here you'd match targets to grid cells, compute CIoU and BCE losses\n",
    "        # For demo purposes, we’ll just simulate basic structure:\n",
    "        obj_target = torch.zeros_like(pred[..., 4], device=device)\n",
    "        cls_target = torch.zeros_like(pred[..., 5], device=device)\n",
    "\n",
    "        # Example dummy (no targets)\n",
    "        box_loss = torch.tensor(0.0, device=device)\n",
    "        obj_loss = bce(torch.sigmoid(pred[..., 4]), obj_target)\n",
    "        cls_loss = bce(torch.sigmoid(pred[..., 5]), cls_target)\n",
    "\n",
    "        total_box_loss += box_loss\n",
    "        total_obj_loss += obj_loss\n",
    "        total_cls_loss += cls_loss\n",
    "\n",
    "    total_loss = lambda_box * total_box_loss + lambda_obj * total_obj_loss + lambda_cls * total_cls_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cabb86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 35150.421875\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.randn(1, 3, 640, 640).to(device)\n",
    "model = BacteriaDetector(num_classes=1).to(device)\n",
    "anchors = [\n",
    "    [(10,13), (16,30), (33,23)],\n",
    "    [(30,61), (62,45), (59,119)],\n",
    "    [(116,90), (156,198), (373,326)]\n",
    "]\n",
    "\n",
    "preds = model(x)\n",
    "loss = detection_loss(preds, targets=[], anchors=anchors, device=device)\n",
    "print(\"Total loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77d9a875",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BacteriaDetector' from 'model' (/Users/ashithrai/Documents/div/model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BacteriaDetector, detection_loss\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BacteriaDetector' from 'model' (/Users/ashithrai/Documents/div/model.py)"
     ]
    }
   ],
   "source": [
    "from model import BacteriaDetector, detection_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "702f2fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ashithrai/Documents/div\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8150bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
